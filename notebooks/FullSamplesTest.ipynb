{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: Some outputs and print messages remain in Italian, as the notebook was translated after execution. I apologize for any mismatches between code and output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bT4HU3PFxqcL"
   },
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnVtyPrSxu6T"
   },
   "source": [
    "To begin, the environment was configured by installing Spark and Parselmouth, and mounting the Google Drive containing the MP3 audio files and the accompanying text document with the transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sH_8AU9OxN32",
    "outputId": "e3d6b2bc-5540-431a-ebc3-71307a293315"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-06 14:25:12--  https://downloads.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n",
      "Resolving downloads.apache.org (downloads.apache.org)... 88.99.208.237, 135.181.214.104, 2a01:4f9:3a:2c57::2, ...\n",
      "Connecting to downloads.apache.org (downloads.apache.org)|88.99.208.237|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 400724056 (382M) [application/x-gzip]\n",
      "Saving to: ‘spark-3.5.5-bin-hadoop3.tgz’\n",
      "\n",
      "spark-3.5.5-bin-had 100%[===================>] 382.16M  12.8MB/s    in 30s     \n",
      "\n",
      "2025-03-06 14:25:43 (12.6 MB/s) - ‘spark-3.5.5-bin-hadoop3.tgz’ saved [400724056/400724056]\n",
      "\n",
      "<module 'pyspark.version' from '/content/spark-3.5.5-bin-hadoop3/python/pyspark/version.py'>\n"
     ]
    }
   ],
   "source": [
    "#Spark installation\n",
    "\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget https://downloads.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.5-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.5-bin-hadoop3\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "print(pyspark.version)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JG4H9xGIyDz1",
    "outputId": "169c0572-0c2e-4a67-e6f9-7923a0ee24f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praat-parselmouth\n",
      "  Downloading praat_parselmouth-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from praat-parselmouth) (1.26.4)\n",
      "Downloading praat_parselmouth-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: praat-parselmouth\n",
      "Successfully installed praat-parselmouth-0.4.5\n"
     ]
    }
   ],
   "source": [
    "#Parselmouth installation\n",
    "\n",
    "!pip install praat-parselmouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YoNHFtBDyD8x",
    "outputId": "a2b7426b-3545-438e-b8d1-a817702ba474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#Drive mounting\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ-n7nujyYpk"
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJTEIXfIyd_b"
   },
   "source": [
    "Subsequently, three functions were implemented to extract the features required to build the samples, along with a function that retrieves the transcriptions from the text file and stores them in a dictionary.\n",
    "\n",
    "The functions can also be found in the `src` folder with further documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpT4zg4bytSc"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.feature\n",
    "import numpy as np\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Extracts audio features with Librosa\n",
    "def extract_audio_features(audio_path, sr=22050, n_mfcc=13):\n",
    "    y, sr = librosa.load(audio_path, sr=sr)\n",
    "\n",
    "    # Extracts MFCC\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)  # Mean to stabilize data\n",
    "\n",
    "    # Extracts RMSE (Energy)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    rms_mean = np.mean(rms)\n",
    "\n",
    "    return np.concatenate((mfccs_mean, [rms_mean]))\n",
    "\n",
    "# Extracts pitch with Parselmouth\n",
    "def extract_pitch(audio_path):\n",
    "    snd = parselmouth.Sound(audio_path)\n",
    "    pitch = call(snd, \"To Pitch\", 0.0, 75, 600)\n",
    "    mean_pitch = call(pitch, \"Get mean\", 0, 0, \"Hertz\")  # Mean\n",
    "\n",
    "    return np.array([mean_pitch])\n",
    "\n",
    "# Obteins textual representation (TF-IDF)\n",
    "def extract_text_features(text, vectorizer):\n",
    "    return vectorizer.transform([text]).toarray()[0]\n",
    "\n",
    "# Extracts transcriptions from .txt file\n",
    "def load_transcriptions(txt_file):\n",
    "    transcriptions = {}\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.read().strip().split(\"\\n\\n\")  # File is divided in blocks thanks to empty lines\n",
    "\n",
    "    for block in lines:\n",
    "        lines = block.split(\"\\n\")  # Each block contains file name + transcription\n",
    "        if len(lines) >= 2:\n",
    "            filename = lines[0].strip()  # First row: mp3 file name\n",
    "            transcript = \" \".join(lines[1:]).strip()  # Everything else is the transcription\n",
    "            transcriptions[filename] = transcript  # Adds to dictionary\n",
    "\n",
    "    return transcriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WN6aCd2zUx7"
   },
   "source": [
    "# Complete samples creation (Librosa + Parselmouth + TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Scu9xCaDzbK7"
   },
   "source": [
    "In this phase, I created the complete samples and saved them in a `.pkl` file on Google Drive (the file is also available in the repository)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wObrlg-z63r"
   },
   "source": [
    "Please note that the folder containing all the audio files and transcriptions is named 'audiozzi'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tt-8EFwzz86",
    "outputId": "0a187019-57a6-4639-be0d-17c4c840f4db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 campioni salvati.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "audio_folder = \"/content/drive/MyDrive/audiozzi\"\n",
    "transcriptions_file = \"/content/drive/MyDrive/audiozzi/Trascrizioni.txt\"\n",
    "\n",
    "# Loads transcriptions in dictionary\n",
    "transcriptions = load_transcriptions(transcriptions_file)\n",
    "\n",
    "# Creates TF-IDF vectorizer and fits it on transcriptions\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(list(transcriptions.values()))\n",
    "\n",
    "# Final data list\n",
    "dataset = []\n",
    "\n",
    "# Audio file in folder loop\n",
    "for file in os.listdir(audio_folder):\n",
    "    if file.endswith(\".mp3\"):  # Just mp3 files\n",
    "        audio_path = os.path.join(audio_folder, file)\n",
    "\n",
    "        # Sets label (1 = Urgent, 0 = Normal) (see file naming in the report)\n",
    "        label = 1 if file.endswith(\"u.mp3\") else 0\n",
    "\n",
    "        # Retrieves transcription\n",
    "        transcript = transcriptions.get(file, None)\n",
    "        if transcript is None:\n",
    "            print(f\"No transcription for {file}, skipped.\")    # Helps for errors in txt file\n",
    "            continue\n",
    "\n",
    "        # Extracts audio features\n",
    "        audio_features = extract_audio_features(audio_path)\n",
    "        pitch_feature = extract_pitch(audio_path)\n",
    "\n",
    "        # Extracts textual features\n",
    "        text_features = extract_text_features(transcript, vectorizer)\n",
    "\n",
    "        # Concatenates features\n",
    "        sample = np.concatenate((audio_features, pitch_feature, text_features))\n",
    "\n",
    "        # Saves data\n",
    "        dataset.append({\n",
    "            \"filename\": file,\n",
    "            \"features\": sample,\n",
    "            \"transcript\": transcript,\n",
    "            \"label\": label\n",
    "        })\n",
    "\n",
    "# Converts to dataframe for storing\n",
    "df = pd.DataFrame(dataset)\n",
    "df.to_pickle(\"/content/drive/MyDrive/audiozzi/samplesCompleti.pkl\")  # Saves in binary file\n",
    "#df.to_csv(\"/content/drive/MyDrive/audiozzi/aaaa.csv\", index=False)\n",
    "\n",
    "print(f\"{len(df)} saved samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMUEDFu00p3M"
   },
   "source": [
    "# Test dei modelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05gmD4N00vc6"
   },
   "source": [
    "In this phase, I loaded the samples from the binary file (this step is not necessary if the samples were just generated in the same notebook).\n",
    "\n",
    "I then split the dataset into a training set and a test set (80-20 split).\n",
    "\n",
    "Finally, I applied three models: Random Forest, Logistic Regression, and Gradient Boosted Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3EDOM1B1YFc"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Loads dataset from binary file\n",
    "df = pd.read_pickle(\"/content/drive/MyDrive/audiozzi/samplesCompleti.pkl\")\n",
    "\n",
    "# Converts to Spark DataFrame with DenseVector\n",
    "spark_df = spark.createDataFrame([\n",
    "    Row(filename=row[\"filename\"],\n",
    "        features=DenseVector(row[\"features\"]),  \n",
    "        label=int(row[\"label\"]))  \n",
    "    for _, row in df.iterrows()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujP9IJvq1ju0"
   },
   "outputs": [],
   "source": [
    "#Split training and test sets 80-20\n",
    "\n",
    "train_df, test_df = spark_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3A6d8MTH1qOb",
    "outputId": "eea1b505-67e2-4fb3-be1b-788379e3d20b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza del modello Random Forest: 98.08%\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Defines Random Forest model\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50)\n",
    "\n",
    "# Trains the model\n",
    "model = rf.fit(train_df)\n",
    "\n",
    "# Generates predictions on test set\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "#predictions.select(\"features\", \"label\", \"prediction\").show(10)\n",
    "\n",
    "# Defines accuracy evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Calculates accuracy\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Random Forest Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bhZmsG3Z1vKE",
    "outputId": "b96a71e7-ab48-4322-cbe6-3e6cdb3a5c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza Logistic Regression: 100.00%\n"
     ]
    }
   ],
   "source": [
    "#Logistic regression\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Defines Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Trains the model\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Predictions\n",
    "lr_predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluation\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
    "print(f\"Logistic Regression Accuracy: {lr_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tER0N_zO1yAT",
    "outputId": "4f79649d-6353-465e-b800-c33d84a7512e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza Gradient-Boosted Trees: 96.15%\n"
     ]
    }
   ],
   "source": [
    "#gradient boosting trees\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Defines model\n",
    "gbt = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=50)\n",
    "\n",
    "# training\n",
    "gbt_model = gbt.fit(train_df)\n",
    "\n",
    "# predictions\n",
    "gbt_predictions = gbt_model.transform(test_df)\n",
    "\n",
    "# evaluation\n",
    "gbt_accuracy = evaluator.evaluate(gbt_predictions)\n",
    "print(f\"Gradient-Boosted Trees Accuracy: {gbt_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
