{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclaimer: Some outputs and print messages remain in Italian, as the notebook was translated after execution. I apologize for any mismatches between code and output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZKs9lTfo_Rg"
   },
   "source": [
    "# Environment setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMkHyEJ4pD4w"
   },
   "source": [
    "Initially, I repeated all the steps already performed and described in the notebook with the complete samples, namely:\n",
    "\n",
    "- Installed Spark and Parselmouth\n",
    "- Mounted Google Drive\n",
    "- Added the utility functions\n",
    "- Defined the folder paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MCvln2rJo4rP",
    "outputId": "c818c787-72da-43ee-b3ee-868268eef80b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-06 19:17:02--  https://downloads.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n",
      "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.208.237, 2a01:4f8:10a:39da::2, ...\n",
      "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 400724056 (382M) [application/x-gzip]\n",
      "Saving to: ‘spark-3.5.5-bin-hadoop3.tgz’\n",
      "\n",
      "spark-3.5.5-bin-had 100%[===================>] 382.16M  28.1MB/s    in 14s     \n",
      "\n",
      "2025-03-06 19:17:16 (26.6 MB/s) - ‘spark-3.5.5-bin-hadoop3.tgz’ saved [400724056/400724056]\n",
      "\n",
      "<module 'pyspark.version' from '/content/spark-3.5.5-bin-hadoop3/python/pyspark/version.py'>\n"
     ]
    }
   ],
   "source": [
    "#Spark installation\n",
    "\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget https://downloads.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.5-bin-hadoop3.tgz\n",
    "!pip install -q findspark\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.5-bin-hadoop3\"\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "print(pyspark.version)\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4HBHuQAphLY",
    "outputId": "60570ebe-d033-45ed-a8c3-64dc54f42f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praat-parselmouth\n",
      "  Downloading praat_parselmouth-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from praat-parselmouth) (1.26.4)\n",
      "Downloading praat_parselmouth-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: praat-parselmouth\n",
      "Successfully installed praat-parselmouth-0.4.5\n"
     ]
    }
   ],
   "source": [
    "#Parselmouth installation\n",
    "\n",
    "!pip install praat-parselmouth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HXnPAF5Ppja_",
    "outputId": "c8df1998-833d-4b2c-a8ce-4013eafcecd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#Drive mounting\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LxhLPKrupmPv"
   },
   "outputs": [],
   "source": [
    "#Utility functions\n",
    "\n",
    "import librosa\n",
    "import librosa.feature\n",
    "import numpy as np\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Audio features extraction\n",
    "def extract_audio_features(audio_path, sr=22050, n_mfcc=13):\n",
    "    y, sr = librosa.load(audio_path, sr=sr)\n",
    "\n",
    "    #MFCC\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)  # mean\n",
    "\n",
    "    #RMSE (Energy)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    rms_mean = np.mean(rms)\n",
    "\n",
    "    return np.concatenate((mfccs_mean, [rms_mean]))\n",
    "\n",
    "# pitch extraction\n",
    "def extract_pitch(audio_path):\n",
    "    snd = parselmouth.Sound(audio_path)\n",
    "    pitch = call(snd, \"To Pitch\", 0.0, 75, 600)\n",
    "    mean_pitch = call(pitch, \"Get mean\", 0, 0, \"Hertz\")  # mean\n",
    "\n",
    "    return np.array([mean_pitch])\n",
    "\n",
    "# textual feature extraction\n",
    "def extract_text_features(text, vectorizer):\n",
    "    return vectorizer.transform([text]).toarray()[0]\n",
    "\n",
    "# transcriptions extraction from txt file\n",
    "def load_transcriptions(txt_file):\n",
    "    transcriptions = {}\n",
    "    with open(txt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.read().strip().split(\"\\n\\n\")  # block split\n",
    "    for block in lines:\n",
    "        lines = block.split(\"\\n\")  # Each block: file name + transcription\n",
    "        if len(lines) >= 2:\n",
    "            filename = lines[0].strip()  # file name\n",
    "            transcript = \" \".join(lines[1:]).strip()  # transcription\n",
    "            transcriptions[filename] = transcript  # Add to dictionary\n",
    "\n",
    "    return transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MYhS15CDp7gQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "audio_folder = \"/content/drive/MyDrive/audiozzi\"\n",
    "transcriptions_file = \"/content/drive/MyDrive/audiozzi/Trascrizioni.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCk3LKfNqhOO"
   },
   "source": [
    "# Samples creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2IpJxp7qj7Z"
   },
   "source": [
    "Subsequently, I created two sets of samples:\n",
    "\n",
    "- One using only Librosa and Parselmouth to extract features from the audio files.\n",
    "- One using only TF-IDF to extract features from the transcriptions.\n",
    "\n",
    "In both cases, the samples were saved as `.pkl` files to be used in the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FqAPbL-urDrI",
    "outputId": "c466047a-e578-4348-b01f-4c39da8da917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 campioni salvati.\n"
     ]
    }
   ],
   "source": [
    "# Sample creation (just librosa and parselmouth)\n",
    "\n",
    "# Final data list\n",
    "datasetlp = []\n",
    "\n",
    "# Loop over every file in the folder\n",
    "for file in os.listdir(audio_folder):\n",
    "    if file.endswith(\".mp3\"):  # mp3 only\n",
    "        audio_path = os.path.join(audio_folder, file)\n",
    "\n",
    "        # Label setting based on file name\n",
    "        label = 1 if file.endswith(\"u.mp3\") else 0\n",
    "\n",
    "        # Audio feature extraction\n",
    "        audio_features = extract_audio_features(audio_path)\n",
    "        pitch_feature = extract_pitch(audio_path)\n",
    "\n",
    "        # Concatenating\n",
    "        sample = np.concatenate((audio_features, pitch_feature))\n",
    "\n",
    "        # Saves data\n",
    "        datasetlp.append({\n",
    "            \"filename\": file,\n",
    "            \"features\": sample,\n",
    "            \"label\": label  # adds label\n",
    "        })\n",
    "\n",
    "# DataFrame conversion for storage\n",
    "dflp = pd.DataFrame(datasetlp)\n",
    "dflp.to_pickle(\"/content/drive/MyDrive/audiozzi/samplesSoloLibrosaParselmouth.pkl\")  # binary file\n",
    "#df.to_csv(\"/content/drive/MyDrive/audiozzi/sss.csv\", index=False)\n",
    "\n",
    "print(f\"{len(dflp)} saved samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-fiU9zzbrqvB",
    "outputId": "b467147a-39ca-491c-ca05-d8f78fcd7b0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 campioni salvati.\n"
     ]
    }
   ],
   "source": [
    "# Sample creation (tf-idf only)\n",
    "\n",
    "# Loads transcriptions\n",
    "transcriptions = load_transcriptions(transcriptions_file)\n",
    "\n",
    "# Creates and fits vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(list(transcriptions.values()))\n",
    "\n",
    "datasettf = []\n",
    "\n",
    "# Loop over every file in audio folder\n",
    "for file in os.listdir(audio_folder):\n",
    "    if file.endswith(\".mp3\"):  # mp3 only\n",
    "        audio_path = os.path.join(audio_folder, file)\n",
    "\n",
    "        # Label setting\n",
    "        label = 1 if file.endswith(\"u.mp3\") else 0\n",
    "\n",
    "        # Retrieves transcription\n",
    "        transcript = transcriptions.get(file, None)\n",
    "        if transcript is None:\n",
    "            print(f\"No transcription for {file}, skipped.\")\n",
    "            continue\n",
    "\n",
    "        # Extracts text features\n",
    "        sample = extract_text_features(transcript, vectorizer)\n",
    "\n",
    "        # Saves data\n",
    "        datasettf.append({\n",
    "            \"filename\": file,\n",
    "            \"features\": sample,\n",
    "            \"transcript\": transcript,\n",
    "            \"label\": label  # Adds label\n",
    "        })\n",
    "\n",
    "# DataFrame conversion\n",
    "dftf = pd.DataFrame(datasettf)\n",
    "dftf.to_pickle(\"/content/drive/MyDrive/audiozzi/samplesSoloTFIDF.pkl\")  # pkl\n",
    "#dftf.to_csv(\"/content/drive/MyDrive/audiozzi/ssssssss.csv\", index=False)\n",
    "\n",
    "print(f\"{len(dftf)} saved samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LPRELoctJ8a"
   },
   "source": [
    "# Model testing and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MuheaZftPBH"
   },
   "source": [
    "Finally, I loaded the samples from the binary files and trained and tested the models to compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQfIL_8jtwPg"
   },
   "source": [
    "### Librosa and Parselmouth only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xBlhZnWptac3"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Loads dataset from pkl file\n",
    "dflp = pd.read_pickle(\"/content/drive/MyDrive/audiozzi/samplesSoloLibrosaParselmouth.pkl\")\n",
    "\n",
    "# DataFrame conversion\n",
    "spark_df = spark.createDataFrame([\n",
    "    Row(filename=row[\"filename\"],\n",
    "        features=DenseVector(row[\"features\"]),  # DenseVector conversion\n",
    "        label=int(row[\"label\"]))  # Label check\n",
    "    for _, row in dflp.iterrows()\n",
    "])\n",
    "\n",
    "# dataset split 80-20\n",
    "\n",
    "train_df_lp, test_df_lp = spark_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8eMruaXt2w6",
    "outputId": "a3f3eb92-e234-4cd0-c050-860b6b8ef815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza del modello Random Forest: 94.23%\n",
      "Accuratezza Logistic Regression: 98.08%\n",
      "Accuratezza Gradient-Boosted Trees: 98.08%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# Random forest \n",
    "rf_lp = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50)\n",
    "\n",
    "# Model training\n",
    "model = rf_lp.fit(train_df_lp)\n",
    "\n",
    "# predictions generation\n",
    "predictions = model.transform(test_df_lp)\n",
    "\n",
    "#predictions.select(\"features\", \"label\", \"prediction\").show(10)\n",
    "\n",
    "# Evaluator setting\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Accuracy \n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Random Forest accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Logistic regression\n",
    "lr_lp = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Model training\n",
    "lr_model = lr_lp.fit(train_df_lp)\n",
    "\n",
    "# Predictions\n",
    "lr_predictions = lr_model.transform(test_df_lp)\n",
    "\n",
    "# Evaluation\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
    "print(f\"Logistic Regression accuracy: {lr_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Gradient Boosted Trees\n",
    "gbt_lp = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=50)\n",
    "\n",
    "# Training\n",
    "gbt_model = gbt_lp.fit(train_df_lp)\n",
    "\n",
    "# predictions\n",
    "gbt_predictions = gbt_model.transform(test_df_lp)\n",
    "\n",
    "# Evaluation\n",
    "gbt_accuracy = evaluator.evaluate(gbt_predictions)\n",
    "print(f\"Gradient-Boosted Trees accuracy: {gbt_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEZJdFM4t6-4"
   },
   "source": [
    "### TF-IDF only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rRLB6pl6t8_Z"
   },
   "outputs": [],
   "source": [
    "# Loads dataset\n",
    "dftf = pd.read_pickle(\"/content/drive/MyDrive/audiozzi/samplesSoloTFIDF.pkl\")\n",
    "\n",
    "# DataFrame conversion\n",
    "spark_df = spark.createDataFrame([\n",
    "    Row(filename=row[\"filename\"],\n",
    "        features=DenseVector(row[\"features\"]),  # DenseVector\n",
    "        label=int(row[\"label\"]))  # label check\n",
    "    for _, row in dftf.iterrows()\n",
    "])\n",
    "\n",
    "#dataset split\n",
    "\n",
    "train_df_tf, test_df_tf = spark_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zlgxZ-4uILo",
    "outputId": "53804c7e-3dc3-4fbe-abb9-9e1519345b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuratezza del modello Random Forest: 100.00%\n",
      "Accuratezza Logistic Regression: 100.00%\n",
      "Accuratezza Gradient-Boosted Trees: 90.38%\n"
     ]
    }
   ],
   "source": [
    "# Random forest\n",
    "rf_tf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50)\n",
    "\n",
    "# Training\n",
    "model = rf_tf.fit(train_df_tf)\n",
    "\n",
    "# Predictions\n",
    "predictions = model.transform(test_df_tf)\n",
    "\n",
    "#predictions.select(\"features\", \"label\", \"prediction\").show(10)\n",
    "\n",
    "# Evaluator setting\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Evaluation\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Random Forest accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# Logistic regression\n",
    "lr_tf = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Training\n",
    "lr_model = lr_tf.fit(train_df_tf)\n",
    "\n",
    "# predictions\n",
    "lr_predictions = lr_model.transform(test_df_tf)\n",
    "\n",
    "# Evaluation\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
    "print(f\"Logistic Regression accuracy: {lr_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "# GBT\n",
    "gbt_tf = GBTClassifier(featuresCol=\"features\", labelCol=\"label\", maxIter=50)\n",
    "\n",
    "# Training\n",
    "gbt_model = gbt_tf.fit(train_df_tf)\n",
    "\n",
    "# predictions\n",
    "gbt_predictions = gbt_model.transform(test_df_tf)\n",
    "\n",
    "# Evaluation\n",
    "gbt_accuracy = evaluator.evaluate(gbt_predictions)\n",
    "print(f\"Gradient-Boosted Trees accuracy: {gbt_accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
